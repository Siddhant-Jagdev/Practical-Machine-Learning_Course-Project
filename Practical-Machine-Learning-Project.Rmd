---
title: "Pracrtical Machine Learning Project"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Practical Machine Learning Project

## Background

Using devices such as Nike FuelBand, Jawbone Up, and Fitbit it is possible to collect large amounts of data about personal activity relatively inexpensively. These type of devices are part of the quantification of movement information. A group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or they are just tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt,arm, forearm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

## Data

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

## Analysis
Loading data:

```{r load data, warning=FALSE, message=FALSE, echo=TRUE}
library(ggplot2)
library(caret)
library(fscaret)
library(randomForest)
library(e1071)

url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url_train, destfile = "pml-training.csv")
download.file(url_test, destfile = "pml-testing.csv")
training <- read.table("pml-training.csv", sep = ",", header = TRUE)
testing <- read.table("pml-testing.csv", sep = ",", header = TRUE)
```
## Splitting the data and selecting features

```{r Splitting the data and selecting features, warning=FALSE, message=FALSE}

set.seed(333)

#We need to split the original training set into our training set and a validation set.
inTrain <- createDataPartition(y=training$classe, p=0.7, list=F)
training1 <- training[inTrain, ]
training2 <- training[-inTrain, ]
# Many columns of the data set contain the same value accros the lines. These “near-zero variance predictors”" bring almost no information to our model and will make computing unnecessarily longer. Others are entirely filled with NA values. Finnaly, the six first variables do not concern fitness motions whatsoever. They also need to be remove before we start fitting our model.

#removing near-zero variance predictors
nzv <- nearZeroVar(training)
training1 <- training1[, -nzv]
training2 <- training2[, -nzv]

# Removing predictors with NA values
training1 <- training1[, colSums(is.na(training1)) == 0]
training2 <- training2[, colSums(is.na(training2)) == 0]
#removing columns unfit for prediction (ID, user_name, raw_timestamp_part_1 etc ...)
training1 <- training1[, -(1:5)]
training2 <- training2[, -(1:5)]
```
## Selecting a model

```{r Selected Model, warning=FALSE, message=FALSE}
# We chose to fit a random forest model. This model provided the most accurate results all along the machine learning course.The cross-validation is set to draw a subset of the data three different times.

mod1 <- train(classe ~., method = "rf", data = training1, verbose = TRUE, trControl = trainControl(method="cv"), number = 3)
pred1 <- predict(mod1, training1)
confusionMatrix(pred1, training1$classe)

# We get a very high accuracy of 99% but we still need to know how this model performs against the test set before expressing a conclusion.

pred12 <- predict(mod1, training2)
confusionMatrix(pred12, training2$classe)

# As we can see we still get a very high accuracy. We didn’t overfit the model when training it.
```
